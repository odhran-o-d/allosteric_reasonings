{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.DistMult import DistMult\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\triplets.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\triplet_lookup.pickle', 'rb') as handle:\n",
    "    lookup = pickle.load(handle)\n",
    "\n",
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\ASD_dictionary.pickle', 'rb') as handle:\n",
    "    ASD_dictionary = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = len(lookup) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DistMult(\n  (emb_e): Embedding(70551, 2, padding_idx=0)\n  (emb_rel): Embedding(3, 2, padding_idx=0)\n  (inp_drop): Dropout(p=0.9, inplace=False)\n  (loss): CrossEntropyLoss()\n)\n"
    }
   ],
   "source": [
    "model = DistMult(num_entities = entities, embedding_dim=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3, 2])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "len(params)\n",
    "params[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([ 6000,   200, 70000,  5000])\ntensor([1, 2, 1, 1])\n"
    }
   ],
   "source": [
    "objects = torch.LongTensor([6000,200,70000,5000])\n",
    "relationships = torch.LongTensor([1,2,1,1])\n",
    "print(objects)\n",
    "print(relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., -0.],\n        [0., 0.],\n        [0., 0.],\n        [0., -0.]], grad_fn=<MulBackward0>)\ntensor([[ 0.0000,  1.1757, -0.3567,  ..., -0.7187,  1.2659, -0.3215],\n        [ 0.0000, -0.0183, -1.0365,  ...,  3.2094, -0.5178,  0.1654]],\n       grad_fn=<TransposeBackward0>)\n"
    }
   ],
   "source": [
    "pred = model(e1 = objects, rel = relationships)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n       grad_fn=<SigmoidBackward>)\n"
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'input' and 'target'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-443c57988fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\.alosteric_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'input' and 'target'"
     ]
    }
   ],
   "source": [
    "#okay so just feed in your embeddings here I guess! \n",
    "model.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([4, 70551])"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "out.size()\n",
    "#okay this makes sense its spitting out a prediction!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n       grad_fn=<SigmoidBackward>)\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DistMult(\n  (emb_e): Embedding(70551, 2, padding_idx=0)\n  (emb_rel): Embedding(3, 2, padding_idx=0)\n  (inp_drop): Dropout(p=0.9, inplace=False)\n  (loss): CrossEntropyLoss()\n)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitalostericvenvvenve042302d83484289899f1ea331a7aba5",
   "display_name": "Python 3.8.2 64-bit ('.alosteric_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}