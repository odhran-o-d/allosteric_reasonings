{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os, os.path\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you are currently overriding entries on the same protein in different organisms! This is occuring around 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = os.listdir(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\raw\\ASD\\ASD_Release_062015_XF')\n",
    "file_path = r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\raw\\ASD\\ASD_Release_062015_XF'\n",
    "\n",
    "paths = []\n",
    "for filename in file_directory:\n",
    "    paths.append(os.path.join(file_path,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "ASD_entries = {}\n",
    "missing = []\n",
    "ASD_ID_list_with_duplicates = []\n",
    "for ASD_file in paths:\n",
    "\n",
    "    try:\n",
    "        root = ET.parse(ASD_file).getroot()\n",
    "        ASD_ID = root.find('Organism_ID').text\n",
    "        #Protein_ID = root.find('UniProt_ID').text\n",
    "        #added PDB parser - makes dataset smaller but so be it. \n",
    "        for PDB in root.find('PDB_List'):\n",
    "            if PDB[4].text == 'No':\n",
    "                Protein_ID = PDB[1].text\n",
    "            break\n",
    "        Organism = root.find('Organism').text\n",
    "        Gene = root.find('Gene')[0].text\n",
    "        Gene_ID = root.find('Gene')[2].text\n",
    "        \n",
    "        modulator_list = []\n",
    "\n",
    "        try:\n",
    "            for modulators in root.find('Modulator_List'):\n",
    "                modulator_list.append((modulators[0].text, modulators[1].text))    \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        ASD_entries[ASD_ID] = {\n",
    "            'Protein_ID' : Protein_ID,\n",
    "            'Organism' : Organism,\n",
    "            'Gene' : Gene,\n",
    "            'Gene_ID' : Gene_ID,\n",
    "            'Modulators' : modulator_list\n",
    "        }\n",
    "\n",
    "        ASD_ID_list_with_duplicates.append((ASD_ID))\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        missing.append(ASD_file)\n",
    "\n",
    "print(len(missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code generates logcal tripples with IDs \n",
    "\n",
    "ASD_ID_list = list(dict.fromkeys(ASD_ID_list_with_duplicates))\n",
    "\n",
    "triples_with_IDs = []\n",
    "for protein_ASD in ASD_ID_list:\n",
    "    interactions = ASD_entries[protein_ASD]['Modulators']\n",
    "    for allo_ASD in interactions:\n",
    "        triples_with_IDs.append([allo_ASD[0], allo_ASD[1], protein_ASD])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This generates hash tables for each of the things in the tripples \n",
    "\n",
    "objects = list(set([item[0] for item in triples_with_IDs]))\n",
    "relationships = list(set([item[1] for item in triples_with_IDs]))\n",
    "subjects = list(set([item[2] for item in triples_with_IDs]))\n",
    "\n",
    "subject_map = {}\n",
    "subject_lookup = {}\n",
    "for subject_index, subject in enumerate(subjects):\n",
    "    subject_map[subject] = subject_index\n",
    "    subject_map[subject_index] = subject\n",
    "\n",
    "relationship_map = {}\n",
    "relationship_lookup = {}\n",
    "for relationship_index, relationship in enumerate(relationships):\n",
    "    relationship_map[relationship] = relationship_index+1\n",
    "    relationship_map[relationship_index+1] = relationship\n",
    "\n",
    "object_map = {}\n",
    "object_lookup = {}\n",
    "for object_index, obj in enumerate(objects):\n",
    "    object_map[obj] = object_index\n",
    "    object_lookup[object_index] = obj\n",
    "\n",
    "\n",
    "nodes= list(set([item[0] for item in triples_with_IDs] + [item[2] for item in triples_with_IDs]))\n",
    "node_map = {}\n",
    "for node_index, node in enumerate(nodes):\n",
    "    node_map[node] = node_index\n",
    "    node_map[node_index] = node\n",
    "    max_node = node_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_triplets = []\n",
    "for triplet in triples_with_IDs:\n",
    "    categorical_triplets.append([\n",
    "        object_map[triplet[0]],\n",
    "        relationship_map[triplet[1]],\n",
    "        subject_map[triplet[2]]\n",
    "    ])\n",
    "\n",
    "same_space_triplets = []\n",
    "for triplet in triples_with_IDs:\n",
    "    same_space_triplets.append([\n",
    "        node_map[triplet[0]],\n",
    "        relationship_map[triplet[1]],\n",
    "        node_map[triplet[2]]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "Output = {}\n",
    "for x, y, z in same_space_triplets:\n",
    "    if (x,y) in Output:\n",
    "        Output[(x,y)].append(z)\n",
    "    else:\n",
    "        Output[(x,y)] = [z]\n",
    "\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "\n",
    "for index, (x, y, z) in enumerate(same_space_triplets):\n",
    "    vector = Output[(x,y)]\n",
    "    v_len = len(vector)\n",
    "    r = np.full(v_len, index, dtype=int)\n",
    "    c = np.array(vector)\n",
    "    d = np.ones(v_len)\n",
    "    rows.append(r)\n",
    "    cols.append(c)\n",
    "    data.append(d)\n",
    "\n",
    "rows = np.hstack(rows)\n",
    "cols = np.hstack(cols)\n",
    "data = np.hstack(data)\n",
    "no_nodes = int(len(node_map)/2)\n",
    "no_rows = len(same_space_triplets)\n",
    "\n",
    "sparse_data = csr_matrix((data, (rows, cols)), shape=(no_rows, no_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE_dictionary = {}\n",
    "for index, (o, r, s)  in enumerate(same_space_triplets):\n",
    "    if (o, r) in BCE_dictionary:\n",
    "        pass\n",
    "    else:\n",
    "        BCE_dictionary[(o, r)] = sparse_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code generates an edge-list graph, if ever required\n",
    "\n",
    "nodes= list(set([item[0] for item in triples_with_IDs] + [item[2] for item in triples_with_IDs]))\n",
    "\n",
    "node_hash_map = {}\n",
    "for node_index, node in enumerate(nodes):\n",
    "    node_hash_map[node] = node_index\n",
    "\n",
    "edge_list = []\n",
    "edge_features = []\n",
    "for protein in ASD_ID_list:\n",
    "    interactions = ASD_entries[protein]['Modulators']\n",
    "    for ASD in interactions:\n",
    "        edge_list.append([\n",
    "            node_hash_map[protein],\n",
    "            node_hash_map[ASD[0]]])\n",
    "        edge_features.append(ASD[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "edge_features2 = [[x] for x in edge_features] \n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(edge_features2)\n",
    "\n",
    "one_hots = encoder.transform(edge_features2).toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "print(triples_with_IDs[0])\n",
    "\n",
    "print(same_space_triplets[0])\n",
    "\n",
    "print(categorical_triplets[0])\n",
    "\n",
    "print(edge_list[0])\n",
    "\n",
    "print(edge_features[0])\n",
    "\n",
    "print(one_hots[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('edge_list.pickle', 'wb') as handle:\n",
    "    pickle.dump((edge_list), handle)\n",
    "with open('edge_features.pickle', 'wb') as handle:\n",
    "    pickle.dump((one_hots), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('triplets.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(same_space_triplets), handle)\n",
    "\n",
    "with open('triplet_lookup.pickle', 'wb') as handle:\n",
    "    pickle.dump(node_map, handle)\n",
    "\n",
    "with open('ASD_dictionary.pickle', 'wb') as handle:\n",
    "    pickle.dump(ASD_entries, handle)\n",
    "\n",
    "with open('Sparse_dictionary.pickle', 'wb') as handle:\n",
    "    pickle.dump(BCE_dictionary, handle)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitalostericvenvvenve042302d83484289899f1ea331a7aba5",
   "display_name": "Python 3.8.2 64-bit ('.alosteric_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}