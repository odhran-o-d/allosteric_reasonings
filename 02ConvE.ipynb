{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ConvE import ConvE, ConvE_args\n",
    "from utils.loaders import load_data\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ConvE_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\triplets.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\triplet_lookup.pickle', 'rb') as handle:\n",
    "    lookup = pickle.load(handle)\n",
    "\n",
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\ASD_dictionary.pickle', 'rb') as handle:\n",
    "    ASD_dictionary = pickle.load(handle)\n",
    "\n",
    "\n",
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\Sparse_dictionary.pickle', 'rb') as handle:\n",
    "    BCE_dictionary = pickle.load(handle)\n",
    "\n",
    "\n",
    "entities = int(len(lookup)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batches = 10\n",
    "number_of_epochs = 30\n",
    "x = shuffle(data)\n",
    "test_data = x[:100] #just limit data to the first 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects, all_relationships, all_subjects = np.split(test_data, 3, axis=1)\n",
    "all_objects = torch.LongTensor(all_objects)\n",
    "all_subjects = torch.squeeze(torch.LongTensor(all_subjects))\n",
    "# all_relationships = torch.LongTensor([a+1 for a in all_relationships])\n",
    "all_relationships = torch.LongTensor(all_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "70555 4\n"
    }
   ],
   "source": [
    "model = ConvE(args = args, num_entities = entities, num_relations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch(a):\n",
    "    oof = BCE_dictionary[(a[0],a[1])]\n",
    "    return((oof.todense()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "70555 4\nnumber of correct answers is\n3\nnumber of correct answers is\n15\nnumber of correct answers is\n43\nnumber of correct answers is\n61\nnumber of correct answers is\n79\nnumber of correct answers is\n92\nnumber of correct answers is\n98\nnumber of correct answers is\n100\nnumber of correct answers is\n100\nnumber of correct answers is\n100\nnumber of correct answers is\n100\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-71a29297bbc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;31m# print(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\.alosteric_venv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\.alosteric_venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ConvE(args = args, num_entities = entities, num_relations=4)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "        model.train() #tells pytorch you are training rather than evaluating, so dropout happens\n",
    "\n",
    "        objects, subjects, relationships  = load_data(test_data, number_of_batches)\n",
    "\n",
    "        for index in range(number_of_batches):\n",
    "\n",
    "            obj = torch.LongTensor(objects[index])\n",
    "            rel = torch.LongTensor(relationships[index])\n",
    "            subj = torch.squeeze(torch.LongTensor(subjects[index]))\n",
    "\n",
    "            # obj_sub = np.hstack((objects[index], relationships[index]))\n",
    "            # BCE = np.apply_along_axis(get_torch, axis= 1, arr = obj_sub)\n",
    "            # BCE_subj = torch.LongTensor(BCE).float()\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            pred = model.forward(e1 = obj, rel = rel)\n",
    "\n",
    "            loss2 = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            loss = loss2(pred, subj)\n",
    "            # print(loss)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # print(model.emb_rel(torch.LongTensor([1])))\n",
    "\n",
    "        outputs = model.forward(e1 = all_objects, rel = all_relationships)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        print('number of correct answers is') \n",
    "        print((predicted == all_subjects).sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this below model is BCE - gave up and swapped to CE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "70555 4\ntensor([[ 1243],\n        [ 6826],\n        [15588],\n        [20694],\n        [34531],\n        [49237],\n        [50161],\n        [53472],\n        [66330]])\ntensor(68454)\ntensor([[19157],\n        [65626]])\ntensor(63506)\ntensor([[28401],\n        [47833]])\ntensor(11057)\ntensor([[ 9733],\n        [35103],\n        [38727],\n        [47143],\n        [51620],\n        [53090],\n        [59967]])\ntensor(19026)\ntensor([[ 2822],\n        [13092],\n        [13495],\n        [18849],\n        [24212],\n        [40427],\n        [42836],\n        [58953],\n        [61703]])\ntensor(19026)\ntensor([[  262],\n        [ 2823],\n        [ 4581],\n        [ 7940],\n        [14179],\n        [24116],\n        [37764],\n        [54330],\n        [59125],\n        [60669],\n        [63720],\n        [67113]])\ntensor(18487)\ntensor([[ 9225],\n        [26620],\n        [31448],\n        [33073],\n        [43997],\n        [50237],\n        [51548],\n        [56387],\n        [66924],\n        [67993]])\ntensor(2854)\ntensor([[  262],\n        [ 2823],\n        [ 4581],\n        [ 7940],\n        [14179],\n        [24116],\n        [37764],\n        [54330],\n        [59125],\n        [60669],\n        [63720],\n        [67113]])\ntensor(914)\ntensor([[ 1243],\n        [ 6826],\n        [15588],\n        [20694],\n        [34531],\n        [49237],\n        [50161],\n        [53472],\n        [66330]])\ntensor(61236)\ntensor([[ 9733],\n        [35103],\n        [38727],\n        [47143],\n        [51620],\n        [53090],\n        [59967]])\ntensor(13343)\ntensor([[  262],\n        [ 1193],\n        [ 2823],\n        [ 4581],\n        [ 7940],\n        [14179],\n        [15671],\n        [24116],\n        [37764],\n        [54330],\n        [59125],\n        [60669],\n        [63720],\n        [67113]])\ntensor(56428)\ntensor([[19458]])\ntensor(2854)\ntensor([[ 9225],\n        [33073]])\ntensor(799)\ntensor([[ 6811],\n        [33307]])\ntensor(54042)\ntensor([[ 9733],\n        [35103],\n        [38727],\n        [47143],\n        [51620],\n        [53090],\n        [59967]])\ntensor(60347)\ntensor([[35432],\n        [53803],\n        [54563]])\ntensor(19552)\ntensor([[ 2822],\n        [13092],\n        [13495],\n        [18849],\n        [24212],\n        [40427],\n        [42836],\n        [58953],\n        [61703]])\ntensor(51620)\ntensor([[19297],\n        [39614],\n        [50878],\n        [53778]])\ntensor(15671)\ntensor([[ 1243],\n        [ 6826],\n        [15588],\n        [20694],\n        [34531],\n        [49237],\n        [50161],\n        [53472],\n        [66330]])\ntensor(60669)\ntensor([[  262],\n        [ 2823],\n        [ 4581],\n        [ 7940],\n        [14179],\n        [24116],\n        [37764],\n        [54330],\n        [59125],\n        [60669],\n        [63720],\n        [67113]])\ntensor(38624)\ntensor([[44963]])\ntensor(799)\ntensor([[ 9733],\n        [35103],\n        [38727],\n        [47143],\n        [51620],\n        [53090],\n        [59967]])\ntensor(59967)\ntensor([[28401],\n        [47833]])\ntensor(60347)\ntensor([[51846]])\ntensor(10664)\ntensor([[19157],\n        [65626]])\ntensor(21342)\ntensor([[ 6811],\n        [33307]])\ntensor(9733)\ntensor([[28401],\n        [47833]])\ntensor(66330)\ntensor([[42396],\n        [57035],\n        [59630]])\ntensor(2822)\ntensor([[44963]])\ntensor(55299)\ntensor([[ 2822],\n        [10418],\n        [13092],\n        [13495],\n        [18849],\n        [24212],\n        [27319],\n        [40427],\n        [42836],\n        [58953],\n        [59145],\n        [61703]])\ntensor(35395)\n"
    }
   ],
   "source": [
    "model = ConvE(args = args, num_entities = entities, num_relations=4)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "        model.train() #tells pytorch you are training rather than evaluating, so dropout happens\n",
    "\n",
    "        objects, subjects, relationships  = load_data(test_data, number_of_batches)\n",
    "\n",
    "        for index in range(number_of_batches):\n",
    "\n",
    "            obj = torch.LongTensor(objects[index])\n",
    "            rel = torch.LongTensor(relationships[index])\n",
    "            subj = torch.squeeze(torch.LongTensor(subjects[index]))\n",
    "\n",
    "            obj_sub = np.hstack((objects[index], relationships[index]))\n",
    "            BCE = np.apply_along_axis(get_torch, axis= 1, arr = obj_sub)\n",
    "            BCE_subj = torch.LongTensor(BCE).float()\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            pred = model.forward(e1 = obj, rel = rel)\n",
    "\n",
    "            loss = model.loss(pred, BCE_subj)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        # print(pred[0].round().nonzero())\n",
    "        print(BCE_subj[0].nonzero())\n",
    "\n",
    "\n",
    "        _, index = torch.max(pred[0], 0)\n",
    "        print(index)\n",
    "\n",
    "        # print(loss)\n",
    "        model.eval()\n",
    "\n",
    "        # print(model.emb_rel(torch.LongTensor([1])))\n",
    "\n",
    "        # outputs = model.forward(e1 = all_objects, rel = all_relationships)\n",
    "        # _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # print('number of correct answers is') \n",
    "        # print((predicted == all_subjects).sum().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model.forward(e1 = all_objects, rel = all_relationships)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[47309],\n        [48579],\n        [63957]])"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "BCE_subj[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = outputs.round()\n",
    "predicted = test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[17108],\n        [32085],\n        [39579],\n        [48866],\n        [55546],\n        [69165]])"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "predicted[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[17358],\n        [23807]])"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "torch.nonzero(work[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(6.)\ntensor(2.)\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
    }
   ],
   "source": [
    "print(sum(predicted[0]))\n",
    "print(sum(work[0]))\n",
    "for index, x in enumerate(predicted):\n",
    "    if torch.all(torch.eq(predicted[index], work[index])) == False:\n",
    "        print(work[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuk = np.hstack((all_objects, all_relationships))\n",
    "please = np.apply_along_axis(get_torch, axis= 1, arr = fuk)\n",
    "work = torch.LongTensor(please).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.forward(e1 = all_objects, rel = all_relationships)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print(predicted)\n",
    "print(predicted/all_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best way to test your system is to train on 100 examples and tests on the 100 same examples - if your model doesn't overfit after a bundle of epochs there is a mistake. \n",
    "# knowledge base models take a long long time to converge. They can take 100s of epochs to converge. YOU SHOULD SHUFFLE YOUR DATA EVERY TIME!!! Don't have the same stuff in each batch. \n",
    "# learn to use the data loader for shuffling. Alternatively do it yourself.  \n",
    "# with BCE you can do label smoothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "\n",
    "print('emb_grad')\n",
    "print(model.emb_e.weight.grad)\n",
    "\n",
    "loss = model.loss(pred, subj)\n",
    "loss.backward()\n",
    "\n",
    "print('emb_grad')\n",
    "print(model.emb_e.weight.grad)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitalostericvenvvenve042302d83484289899f1ea331a7aba5",
   "display_name": "Python 3.8.2 64-bit ('.alosteric_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}