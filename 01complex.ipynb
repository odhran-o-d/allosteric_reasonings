{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Complex import Complex\n",
    "from utils.loaders import load_data\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\triplets.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\triplet_lookup.pickle', 'rb') as handle:\n",
    "    lookup = pickle.load(handle)\n",
    "\n",
    "with open(r'C:\\Users\\odhra\\Documents\\graph_project\\allosteric_reasonings\\data\\processed\\ASD_dictionary.pickle', 'rb') as handle:\n",
    "    ASD_dictionary = pickle.load(handle)\n",
    "\n",
    "entities = int(len(lookup)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batches = 10\n",
    "number_of_epochs = 30\n",
    "x = shuffle(data)\n",
    "test_data = x[:100] #just limit data to the first 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects, all_relationships, all_subjects = np.split(test_data, 3, axis=1)\n",
    "all_objects = torch.LongTensor(all_objects)\n",
    "all_subjects = torch.squeeze(torch.LongTensor(all_subjects))\n",
    "# all_relationships = torch.LongTensor([a+1 for a in all_relationships])\n",
    "all_relationships = torch.LongTensor(all_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model = Complex(num_entities = entities, embedding_dim=100, num_relations=4)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "        model.train() #tells pytorch you are training rather than evaluating, so dropout happens\n",
    "\n",
    "        objects, subjects, relationships  = load_data(test_data, number_of_batches)\n",
    "\n",
    "        for index in range(number_of_batches):\n",
    "\n",
    "            obj = torch.LongTensor(objects[index])\n",
    "            rel = torch.LongTensor(relationships[index])\n",
    "            subj = torch.squeeze(torch.LongTensor(subjects[index]))\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            pred = model.forward(e1 = obj, rel = rel)\n",
    "            loss = model.loss(pred, subj)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # print(model.emb_rel(torch.LongTensor([1])))\n",
    "\n",
    "        outputs = model.forward(e1 = all_objects, rel = all_relationships)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        print('number of correct answers is') \n",
    "        print((predicted == all_subjects).sum().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.forward(e1 = all_objects, rel = all_relationships)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print(predicted)\n",
    "print(predicted/all_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best way to test your system is to train on 100 examples and tests on the 100 same examples - if your model doesn't overfit after a bundle of epochs there is a mistake. \n",
    "# knowledge base models take a long long time to converge. They can take 100s of epochs to converge. YOU SHOULD SHUFFLE YOUR DATA EVERY TIME!!! Don't have the same stuff in each batch. \n",
    "# learn to use the data loader for shuffling. Alternatively do it yourself.  \n",
    "# with BCE you can do label smoothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "\n",
    "print('emb_grad')\n",
    "print(model.emb_e.weight.grad)\n",
    "\n",
    "loss = model.loss(pred, subj)\n",
    "loss.backward()\n",
    "\n",
    "print('emb_grad')\n",
    "print(model.emb_e.weight.grad)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitalostericvenvvenve042302d83484289899f1ea331a7aba5",
   "display_name": "Python 3.8.2 64-bit ('.alosteric_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}